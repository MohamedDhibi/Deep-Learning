{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6042d4e-9a79-4779-adff-1a809e1532d6",
   "metadata": {
    "id": "f6042d4e-9a79-4779-adff-1a809e1532d6"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b175ac1",
   "metadata": {
    "id": "4b175ac1"
   },
   "source": [
    "# Introduction to Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe182d-930e-424b-bcff-5bb95fa216ed",
   "metadata": {
    "id": "e2fe182d-930e-424b-bcff-5bb95fa216ed"
   },
   "source": [
    "## 01 - Introduction to Graphs and Graph Neural Networks ##\n",
    "In this notebook, you will learn the fundamental concepts of graphs and graph neural networks.\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the below sections:\n",
    "1. [Introduction to Graphs](#s1-1)\n",
    "    * [Working With Graph Data](#s1-1.1)\n",
    "    * [Exercise #1 - Building a Simple Graph](#s1-e1)\n",
    "    * [Manipulating Node and Edge Features](#s1-1.2)\n",
    "    * [Graph Data Representation](#s1-1.3)\n",
    "2. [Dataset Overview](#s1-2)\n",
    "    * [Exploratory Data Analysis](#s1-2.1)\n",
    "    * [Exercise #2 - Find Node With Highest Number of Connections](#s1-e2)\n",
    "    * [Data Preparation and Subgraph](#s1-2.2)\n",
    "3. [Building Graph Neural Networks for Node Classification](#s1-3)\n",
    "    * [Message Passing and Graph Convolution](#s1-3.1)\n",
    "4. [Building GNNs With PyTorch](#s1-4)\n",
    "    * [Sum-Pooling](#s1-4.1)\n",
    "    * [Baseline MLP Model](#s1-4.2)\n",
    "    * [Exercise #3 - Mean-Pooling](#s1-e3)\n",
    "    * [GCN, Graph Convolutional Network](#s1-4.3)\n",
    "5. [Building GNNs With DGL's Built-In Modules](#s1-5)\n",
    "    * [GraphConv](#s1-5.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d50743-60f7-4d97-8490-7f0137c2b8b4",
   "metadata": {
    "id": "32d50743-60f7-4d97-8490-7f0137c2b8b4"
   },
   "source": [
    "<a name='s1-1'></a>\n",
    "## Introduction to Graphs ##\n",
    "A graph is a type of data structure that contains **nodes** and **edges**. A node can be a person, place, or thing, and the edges define the relationship between nodes. Edges can be directed, where an edge has a source node and a destination node. They can also be undirected, where there is no notion of source or destination nodes. Graphs are excellent in dealing with complex problems with relationships and interactions.\n",
    "\n",
    "Data that are naturally represented by graphs include:\n",
    "* **Citation networks** can be used to study how publications relate to each other.\n",
    "* **Social networks** are tools to study patterns in collective behavior of people, institutions, and organizations. A social network graph represents groups of people by modelling individuals as nodes, and their relationships as edges.\n",
    "* **Molecules** can be described as a graph, where nodes are atoms and edges are the bonds they share.\n",
    "\n",
    "The structure of graphs can vary greatly in terms of the number of nodes, edges, and the connectivity of nodes.\n",
    "\n",
    "Some of the properties that make graphs different from other types of data include:\n",
    "1. A graph exists in non-Euclidean space, which makes it harder to interpret the data. To visualize the data, there are various dimensionality reduction tools.\n",
    "2. Graphs are unstructured and do not have a fixed form.\n",
    "3. Their large size and high dimensionality increase the complexity for human interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511f98a-2059-4422-964c-fef8ca92399d",
   "metadata": {
    "id": "5511f98a-2059-4422-964c-fef8ca92399d",
    "tags": []
   },
   "source": [
    "<a name='s1-1.1'></a>\n",
    "### Working With Graph Data ###\n",
    "In this lab, we will use the open-source Deep Graph Library, [DGL](https://www.dgl.ai/), and [PyTorch](https://pytorch.org/) to work with graph data. Other popular graph deep learning libraries include [Spektral](https://graphneural.network/), [Graph Nets](https://www.deepmind.com/open-source/graph-nets), and [PyTorch Geometric](https://www.pyg.org/), which allow us to manipulate graph data in similar ways.\n",
    "\n",
    "DGL represents each node by a unique integer, known as a node ID, and each edge by a pair of integers corresponding to the IDs of its end nodes. DGL assigns each edge a unique integer, known as edge ID, based on the order in which it was added to the graph starting with `0`. In DGL, all edges are directed, and an edge `(u, v)` indicates that the direction goes from source node `u` to destination node `v`. When making **undirected** graphs, edges can be treated as **bidirectional** by adding **reverse edges**.\n",
    "\n",
    "We start by creating a sample graph below:\n",
    "\n",
    "<p><img src='images/sample_graph_1.png' width=240></p>\n",
    "\n",
    "A [`DGLGraph`](https://docs.dgl.ai/en/latest/api/python/dgl.DGLGraph.html) can be created with [`dgl.graph(data)`](https://docs.dgl.ai/en/0.9.x/generated/dgl.graph.html), which takes a pair of node IDs `(U, V)` that represent the source node(s) and destination node(s). Once created, we can use [`dgl.DGLGraph.nodes()`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.nodes.html), [`dgl.DGLGraph.edges()`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.edges.html), or [`dgl.DGLGraph.edge_ids(u, v)`](https://docs.dgl.ai/generated/dgl.DGLGraph.edge_ids.html) to reference the nodes and edges. Edges can also be identified using [`dgl.DGLGraph.find_edges(eid)`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.find_edges.html), [`dgl.DGLGraph.in_edges(v)`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.in_edges.html), or [`dgl.DGLGraph.out_edges(u)`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.out_edges.html). For finding the node degree, or the number of connections, we can use [`dgl.DGLGraph.in_degrees(v)`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.in_degrees.html) or [`dgl.DGLGraph.out_degrees(u)`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.out_degrees.html). Optionally, we can use [`networkx`](https://networkx.org/) to visualize the small graphs.\n",
    "\n",
    "_Note: More details about the DGLGraph API, which includes [querying basic graph structure](https://docs.dgl.ai/en/0.7.x/api/python/dgl.DGLGraph.html#querying-graph-structure) properties, can be found [here](https://docs.dgl.ai/en/0.7.x/api/python/dgl.DGLGraph.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697baeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dglNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for dgl from https://files.pythonhosted.org/packages/5a/ff/2ff1e38194aa66ca2d7393415ad11eccbf8d9ea5e74dfaaa2c145d34a240/dgl-1.1.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading dgl-1.1.2-cp311-cp311-win_amd64.whl.metadata (530 bytes)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\hamma\\appdata\\roaming\\python\\python311\\site-packages (from dgl) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from dgl) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.1 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from dgl) (3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from dgl) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from dgl) (4.65.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from dgl) (5.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from tqdm->dgl) (0.4.6)\n",
      "Downloading dgl-1.1.2-cp311-cp311-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB 393.8 kB/s eta 0:00:09\n",
      "    --------------------------------------- 0.1/3.4 MB 550.5 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.1/3.4 MB 585.1 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/3.4 MB 602.4 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.2/3.4 MB 657.6 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.2/3.4 MB 695.5 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.2/3.4 MB 724.0 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.3/3.4 MB 741.6 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.3/3.4 MB 761.4 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.3/3.4 MB 749.8 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.4/3.4 MB 765.8 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.4/3.4 MB 778.2 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.4/3.4 MB 790.7 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.5/3.4 MB 779.5 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.5/3.4 MB 790.3 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.5/3.4 MB 796.1 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.6/3.4 MB 804.9 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.6/3.4 MB 795.2 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.6/3.4 MB 803.1 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.7/3.4 MB 809.4 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.7/3.4 MB 813.9 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.7/3.4 MB 819.2 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.8/3.4 MB 825.1 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 0.8/3.4 MB 816.6 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 0.8/3.4 MB 822.1 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.9/3.4 MB 837.4 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.9/3.4 MB 829.2 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.9/3.4 MB 833.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.9/3.4 MB 828.3 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.4 MB 830.0 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.4 MB 817.2 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.0/3.4 MB 827.4 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.1/3.4 MB 831.4 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.1/3.4 MB 836.9 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.1/3.4 MB 831.1 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.2/3.4 MB 839.8 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.2/3.4 MB 828.1 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.2/3.4 MB 838.7 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.3/3.4 MB 839.7 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.3/3.4 MB 840.6 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.3/3.4 MB 843.6 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.4/3.4 MB 846.4 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.4/3.4 MB 842.9 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.4/3.4 MB 843.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.5/3.4 MB 840.4 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.5/3.4 MB 841.2 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.5/3.4 MB 837.7 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.6/3.4 MB 844.5 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.6/3.4 MB 846.5 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.6/3.4 MB 841.8 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.7/3.4 MB 844.2 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.7/3.4 MB 851.6 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.7/3.4 MB 853.8 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.8/3.4 MB 849.2 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.8/3.4 MB 851.3 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.8/3.4 MB 848.1 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.8/3.4 MB 850.2 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.9/3.4 MB 855.3 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.9/3.4 MB 848.0 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.9/3.4 MB 848.5 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.0/3.4 MB 850.4 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.0/3.4 MB 846.5 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.0/3.4 MB 848.3 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.1/3.4 MB 849.8 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.1/3.4 MB 851.6 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.1/3.4 MB 853.3 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.2/3.4 MB 849.6 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.2/3.4 MB 855.3 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.2/3.4 MB 851.7 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.3/3.4 MB 853.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.3/3.4 MB 854.6 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.3/3.4 MB 851.4 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.4/3.4 MB 856.4 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.4/3.4 MB 853.0 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.4/3.4 MB 854.5 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.4/3.4 MB 852.4 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 2.5/3.4 MB 849.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 2.5/3.4 MB 850.7 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.5/3.4 MB 852.2 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.6/3.4 MB 852.5 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.6/3.4 MB 854.7 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.6/3.4 MB 851.7 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.7/3.4 MB 855.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.7/3.4 MB 856.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.7/3.4 MB 853.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.8/3.4 MB 854.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.8/3.4 MB 859.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.8/3.4 MB 857.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.9/3.4 MB 858.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.9/3.4 MB 858.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.9/3.4 MB 855.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.0/3.4 MB 857.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.0/3.4 MB 857.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.0/3.4 MB 859.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.1/3.4 MB 859.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.1/3.4 MB 860.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.1/3.4 MB 859.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.2/3.4 MB 859.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.2/3.4 MB 857.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.2/3.4 MB 854.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.2/3.4 MB 855.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.3/3.4 MB 856.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.3/3.4 MB 857.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.3/3.4 MB 858.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 855.6 kB/s eta 0:00:00\n",
      "Installing collected packages: dgl\n",
      "Successfully installed dgl-1.1.2\n"
     ]
    }
   ],
   "source": [
    "pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87526336-e6fa-40b1-823d-ed970b8595f3",
   "metadata": {
    "id": "87526336-e6fa-40b1-823d-ed970b8595f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "import dgl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ea4a03-6e3d-493c-80c5-1c0c24b2bc9f",
   "metadata": {
    "id": "a2ea4a03-6e3d-493c-80c5-1c0c24b2bc9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=5, num_edges=3,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create source nodes for edges (2, 1), (3, 2), (4, 3)\n",
    "sample_u=[2, 3, 4]\n",
    "\n",
    "# create destination nodes for edges (2, 1), (3, 2), (4, 3)\n",
    "sample_v=[1, 2, 3]\n",
    "\n",
    "# create graph\n",
    "sample_g=dgl.graph((sample_u, sample_v))\n",
    "\n",
    "# print graph\n",
    "print(sample_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06597dbc-7995-403e-830d-d078bb07beb2",
   "metadata": {
    "id": "06597dbc-7995-403e-830d-d078bb07beb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node IDs are: \n",
      "tensor([0, 1, 2, 3, 4])\n",
      "\n",
      "Source & destination nodes of every edge are: \n",
      "(tensor([2, 3, 4]), tensor([1, 2, 3]))\n",
      "\n",
      "Edge IDs are: \n",
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# print node IDs\n",
    "print(\"Node IDs are: \\n{}\\n\".format(sample_g.nodes()))\n",
    "\n",
    "# print the source and destination nodes of every edge\n",
    "print(\"Source & destination nodes of every edge are: \\n{}\\n\".format(sample_g.edges()))\n",
    "\n",
    "# print edge IDs\n",
    "print(\"Edge IDs are: \\n{}\".format(sample_g.edge_ids(sample_u, sample_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce198789-f5fe-4364-8273-49cd28ca140c",
   "metadata": {
    "id": "ce198789-f5fe-4364-8273-49cd28ca140c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node IDs are: \n",
      "tensor([0, 1, 2, 3, 4])\n",
      "\n",
      "Source & destination nodes of every edge are: \n",
      "(tensor([1, 2, 2, 3, 3, 4]), tensor([2, 1, 3, 2, 4, 3]))\n",
      "\n",
      "Edge IDs for directed graph are: \n",
      "tensor([1, 3, 5])\n",
      "\n",
      "Edge IDs for bidirectional/undirected graph are: \n",
      "tensor([1, 3, 5, 0, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# make bidirected graph\n",
    "sample_g=dgl.to_bidirected(sample_g)\n",
    "\n",
    "# dgl.add_reverse_edges(graph) achieves similar result\n",
    "# sample_g=dgl.add_reverse_edges(sample_g)\n",
    "\n",
    "# print graph properties\n",
    "print(\"Node IDs are: \\n{}\\n\".format(sample_g.nodes()))\n",
    "print(\"Source & destination nodes of every edge are: \\n{}\\n\".format(sample_g.edges()))\n",
    "print(\"Edge IDs for directed graph are: \\n{}\\n\".format(sample_g.edge_ids(sample_u, sample_v)))\n",
    "\n",
    "# print all edges\n",
    "print(\"Edge IDs for bidirectional/undirected graph are: \\n{}\".format(sample_g.edge_ids(sample_u+sample_v, sample_v+sample_u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3965913c-db77-465a-9b76-b585aec97e64",
   "metadata": {
    "id": "3965913c-db77-465a-9b76-b585aec97e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source node(s) tensor([2]) are connected to destination node(s) tensor([1]). \n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# find all edges connected to node 1\n",
    "node_id=1\n",
    "print(\"Source node(s) {0[0]} are connected to destination node(s) {0[1]}. \".format(sample_g.in_edges(node_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d124723-c874-46f8-94c2-4a7109a004db",
   "metadata": {
    "id": "7d124723-c874-46f8-94c2-4a7109a004db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node degrees for all nodes: \n",
      "tensor([0, 1, 2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# get node degrees\n",
    "print(\"Node degrees for all nodes: \\n{}\".format(sample_g.in_degrees()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb1e26d-af21-4ab8-b79c-59a0c9762fd6",
   "metadata": {
    "id": "ebb1e26d-af21-4ab8-b79c-59a0c9762fd6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# visualize the graph\n",
    "import networkx as nx\n",
    "\n",
    "# draw plot using networkx\n",
    "G=dgl.to_networkx(sample_g)\n",
    "nx.draw_networkx(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701ba36-9067-456f-bf79-ad5b1332d13b",
   "metadata": {
    "id": "2701ba36-9067-456f-bf79-ad5b1332d13b"
   },
   "source": [
    "_Note: This graph consists of 5 nodes and 3 edges. The number of nodes is automatically inferred from the max node ID in the given edges. Furthermore, the edge IDs will automatically get numbered based on the order they were added and double-counted for bidirectional/undirected graphs._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df054e-2575-4cbd-ac71-915e2b595833",
   "metadata": {
    "id": "e8df054e-2575-4cbd-ac71-915e2b595833"
   },
   "source": [
    "<a name='s1-e1'></a>\n",
    "### Exercise #1 - Building a Simple Graph ###\n",
    "Let's create the below graph.\n",
    "\n",
    "<p><img src='images/sample_graph_2.png' width=240></p>\n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>`s only to create the sample graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b610d629-32c7-4e82-bd90-4af90f71ed5a",
   "metadata": {
    "id": "b610d629-32c7-4e82-bd90-4af90f71ed5a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create sources nodes and destination nodes\n",
    "u=[0, 1, 2,3,4,5]\n",
    "v=[1, 2,3,4,5,3]\n",
    "g=dgl.graph((u, v))\n",
    "\n",
    "# when making undirected graphs, edges are treated as bidirectional\n",
    "g=dgl.to_bidirected(g)\n",
    "\n",
    "# draw plot using networkx\n",
    "nx.draw_networkx(dgl.to_networkx(g))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7c83bfd-4756-4609-8b05-7406ecc2606d",
   "metadata": {
    "id": "a7c83bfd-4756-4609-8b05-7406ecc2606d",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "u=[0, 1, 2, 3, 3, 4]\n",
    "v=[1, 2, 3, 4, 5, 5]\n",
    "g=dgl.graph((u, v))\n",
    "g=dgl.to_bidirected(g)\n",
    "\n",
    "nx.draw_networkx(dgl.to_networkx(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134cf1cd-ad51-498f-8487-c7b22c71dfb6",
   "metadata": {
    "id": "134cf1cd-ad51-498f-8487-c7b22c71dfb6"
   },
   "source": [
    "<a name='s1-1.2'></a>\n",
    "### Manipulating Node and Edge Features ###\n",
    "Many graph data contain features on nodes and edges. We can assign and retrieve node and edge features via the [`dgl.DGLGraph.ndata`](https://docs.dgl.ai/en/latest/generated/dgl.DGLGraph.ndata.html#dgl.DGLGraph.ndata) and [`dgl.DGLGraph.edata`](https://docs.dgl.ai/en/latest/generated/dgl.DGLGraph.edata.html#dgl.DGLGraph.edata) interfaces similar to how we would add/retrieve key-value pairs in a [Python dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries). `ndata` and `edata` can also be used to store other node-level and edge-level data for deep learning like `labels` and `train`/`test masks`. There can also be global properties about the overall graph.\n",
    "\n",
    "_Note: DGLGraph only accepts features stored as numerical [tensors](https://pytorch.org/docs/stable/tensors.html). The vast development of deep learning has provided us many ways to encode various types of attributes into numerical features._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b2bbc7-5cc8-4fe7-9e4b-325815f6b1d5",
   "metadata": {
    "id": "d7b2bbc7-5cc8-4fe7-9e4b-325815f6b1d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node data: \n",
      "{}\n",
      "\n",
      "Edge data: \n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# print ndata\n",
    "print(\"Node data: \\n{}\\n\".format(g.ndata))\n",
    "\n",
    "# print edata\n",
    "print(\"Edge data: \\n{}\".format(g.edata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcad1af-9e70-41db-8df7-c54f5e7beb7e",
   "metadata": {
    "id": "ebcad1af-9e70-41db-8df7-c54f5e7beb7e"
   },
   "source": [
    "Below we demonstrate assigning random values as node feature vectors. We use the [`dgl.DGLGraph.num_nodes()`](https://docs.dgl.ai/generated/dgl.DGLGraph.num_nodes.html#dgl.DGLGraph.num_nodes) function to get the number of nodes in our graph and then we assign a random multi-dimensional node feature vector called `feat` using `dgl.DGLGraph.ndata`. We can do similar assignments with [`dgl.DGLGraph.num_edges()`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.num_edges.html?highlight=num%20edges#dgl.DGLGraph.num_edges) and `dgl.DGLGraph.edata`.\n",
    "\n",
    "_Note: Node and edge features can be named anything just like how we can have arbitrarily named keys for a Python dictionary. For example, we could have called the node feature `f_n` instead of `feat`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c3b7dcf-568d-41e8-87cd-4524838bb7d6",
   "metadata": {
    "id": "3c3b7dcf-568d-41e8-87cd-4524838bb7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features (torch.Size([6, 4])): \n",
      "{'feat': tensor([[-1.2516, -0.8651, -1.3014,  0.0625],\n",
      "        [ 0.5810,  1.4520, -1.4490, -0.2560],\n",
      "        [-0.6697,  0.2754, -0.2449, -0.4638],\n",
      "        [ 0.6594, -0.3710,  0.7442, -0.1670],\n",
      "        [ 0.8085, -0.2381,  0.9389, -0.5994],\n",
      "        [ 1.1182, -0.6846,  1.1714,  0.6270]])}\n",
      "\n",
      "Edge features (torch.Size([12, 5])): \n",
      "{'f_e': tensor([[-2.1357e-01,  1.3346e+00, -1.2651e+00,  4.2594e-01,  6.8394e-02],\n",
      "        [ 6.5100e-01,  5.6032e-01,  1.1491e+00,  5.6506e-01,  2.1122e-01],\n",
      "        [ 9.4416e-02, -1.0022e+00, -1.0731e+00, -1.2364e+00, -1.1671e+00],\n",
      "        [ 1.6080e+00,  3.1164e-01,  3.1382e-01, -3.6033e-02, -3.8248e-01],\n",
      "        [-1.7259e+00, -8.8785e-01, -1.9103e+00,  6.4537e-01,  1.4849e+00],\n",
      "        [ 2.0944e-01,  9.7583e-01, -1.7074e+00, -2.8119e-01,  1.5387e+00],\n",
      "        [-6.2017e-01,  1.4829e+00,  3.9551e-02, -5.9101e-01, -2.2275e-01],\n",
      "        [-7.8378e-01, -3.5616e-02,  7.9398e-01,  1.4276e-01,  7.0457e-01],\n",
      "        [-4.0937e-01,  2.2888e+00,  1.0011e+00, -6.5605e-01, -4.4220e-01],\n",
      "        [-3.7387e-01,  2.2386e-01, -4.4802e-01,  7.3323e-01, -3.3182e-04],\n",
      "        [-2.0746e+00, -2.2400e-02,  6.7259e-02, -1.1531e+00,  2.0622e-01],\n",
      "        [-1.0676e+00,  1.7379e+00,  2.1029e-01, -1.4326e-01, -7.2084e-01]])}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# get number of nodes\n",
    "num_nodes=g.num_nodes()\n",
    "\n",
    "# assign a 4-dimensional random node feature vector called feat for each node\n",
    "g.ndata['feat']=torch.randn(num_nodes, 4)\n",
    "\n",
    "# print node features\n",
    "print(\"Node features ({}): \\n{}\\n\".format(g.ndata['feat'].shape, g.ndata))\n",
    "\n",
    "# assign a 5-dimensional random edge feature vector called f_e for each edge\n",
    "num_edges=g.num_edges()\n",
    "g.edata['f_e']=torch.randn(num_edges, 5)\n",
    "print(\"Edge features ({}): \\n{}\".format(g.edata['f_e'].shape, g.edata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc7297-1f14-424e-9cc6-28979c5f7993",
   "metadata": {
    "id": "05fc7297-1f14-424e-9cc6-28979c5f7993"
   },
   "source": [
    "<a name='s1-1.3'></a>\n",
    "### Graph Data Representation ###\n",
    "A graph is often represented by an **adjacency matrix**. If a graph has `n` nodes, then the adjacency matrix would have a dimension that is `n` x `n`. The matrix contains `0/1-valued` vectors to indicate whether a connection exists between a source node and a destination node. While [`dgl.DGLGraph.adj()`](https://docs.dgl.ai/en/0.8.x/generated/dgl.DGLGraph.adj.html?highlight=adj) returns a [sparse Tensor](https://pytorch.org/docs/stable/sparse.html), we can further convert it to a dense Tensor so it's more visually intuitive. For our graph, the adjacency matrix looks like the below:\n",
    "\n",
    "<p><img src='images/adj_matrix.png' width=240></p>\n",
    "\n",
    "However, while visualizing graph structure as an adjacency matrix might be convenient and intuitive, it's typically not the most efficient if it's sparse (filled with zeros). Another way we can represent edges is by using **adjacency lists**. They describe the connectivity of edges between nodes as a tuple of 1D tensors `(v, u)`, representing destination and sources nodes of all edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3087a848-f7cc-4c8b-938e-b288500df4ea",
   "metadata": {
    "id": "3087a848-f7cc-4c8b-938e-b288500df4ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix (sparse Tensor): \n",
      "SparseMatrix(indices=tensor([[0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 5, 5],\n",
      "                             [1, 0, 2, 1, 3, 2, 4, 5, 3, 5, 3, 4]]),\n",
      "             values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "             shape=(6, 6), nnz=12)\n",
      "\n",
      "Adjacency matrix (dense Tensor): \n",
      "tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# print adjacency matrix as a sparse Tensor\n",
    "print(\"Adjacency matrix (sparse Tensor): \\n{}\\n\".format(g.adj()))\n",
    "\n",
    "# print adjacency matrix as a dense Tensor\n",
    "print(\"Adjacency matrix (dense Tensor): \\n{}\".format(g.adj().to_dense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0208de1f-06cb-4609-a729-11aed9a292a0",
   "metadata": {
    "id": "0208de1f-06cb-4609-a729-11aed9a292a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node degrees computed manually: \n",
      "tensor([1, 2, 2, 3, 2, 2])\n",
      "\n",
      "Node degrees via DGL API: \n",
      "tensor([1, 2, 2, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# manually calculate node degrees and use .long() to convert to integers\n",
    "g.ndata['degree']=g.adj().to_dense().sum(axis=1).long()\n",
    "\n",
    "# print node degrees\n",
    "print(\"Node degrees computed manually: \\n{}\\n\".format(g.ndata['degree']))\n",
    "\n",
    "# node degrees can also be obtained via DGL API\n",
    "print(\"Node degrees via DGL API: \\n{}\".format(g.in_degrees()))\n",
    "\n",
    "# features can also be deleted, analogous to Dictionary entries\n",
    "del g.ndata['degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2407ba44-0a21-468d-b5e3-db1975d99d4e",
   "metadata": {
    "id": "2407ba44-0a21-468d-b5e3-db1975d99d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: \n",
      "[(0, 1), (1, 0), (1, 2), (2, 1), (2, 3), (3, 2), (3, 4), (3, 5), (4, 3), (4, 5), (5, 3), (5, 4)]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# print destination and source nodes for all edges as adjacency lists\n",
    "u=g.edges()[0].tolist()\n",
    "v=g.edges()[1].tolist()\n",
    "print(\"Edges: \\n{}\".format(list(zip(u, v))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66bb4b-ae9e-4d09-8b7c-52b1996e918c",
   "metadata": {
    "id": "0b66bb4b-ae9e-4d09-8b7c-52b1996e918c"
   },
   "source": [
    "<a name='s1-2'></a>\n",
    "## Dataset Overview ##\n",
    "For a realistic demonstration, we are using the [`ogbn-arxiv`](https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv) dataset from [**O**pen **G**raph **B**enchmark](https://ogb.stanford.edu/). The `ogbn-arxiv` dataset is a directed graph, representing the citation network between Computer Science (CS) arXiv papers. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional [word2vec](https://en.wikipedia.org/wiki/Word2vec) feature vector. It's a great example of how GNNs can be used to leverage the information embedded in graphs, in order to make predictions.\n",
    "\n",
    "Each node belongs to one of the 40 subject areas of arXiv CS papers, e.g., cs.AI, cs.LG, and cs.OS, which are manually determined and labeled by the paper’s authors and arXiv moderators.\n",
    "\n",
    "While some datasets may contain more than 1 graph, the `ogbn-arxiv` dataset contains a single graph at index `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc055448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/78.8 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/78.8 kB ? eta -:--:--\n",
      "     -------------- ----------------------- 30.7/78.8 kB 640.0 kB/s eta 0:00:01\n",
      "     ----------------------------- -------- 61.4/78.8 kB 812.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 78.8/78.8 kB 622.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from ogb) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\hamma\\appdata\\roaming\\python\\python311\\site-packages (from ogb) (1.26.0)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from ogb) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from ogb) (1.3.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from ogb) (2.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from ogb) (1.26.16)\n",
      "Collecting outdated>=0.2.0 (from ogb)\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: setuptools>=44 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from outdated>=0.2.0->ogb) (68.0.0)\n",
      "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->ogb) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->ogb) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from torch>=1.6.0->ogb) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from torch>=1.6.0->ogb) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from torch>=1.6.0->ogb) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from tqdm>=4.29.0->ogb) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hamma\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
      "Building wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py): started\n",
      "  Building wheel for littleutils (setup.py): finished with status 'done'\n",
      "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7034 sha256=cf61694e7fa464403ce3ccf24ff321472b5ebf4638871bb43c68b1b69a239508\n",
      "  Stored in directory: c:\\users\\hamma\\appdata\\local\\pip\\cache\\wheels\\17\\8d\\65\\9a39917567093c895549811c172be5d2dfb63c7e4b143e05a4\n",
      "Successfully built littleutils\n",
      "Installing collected packages: littleutils, outdated, ogb\n",
      "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b005379-1352-42b6-95fc-997bb5be9749",
   "metadata": {
    "id": "2b005379-1352-42b6-95fc-997bb5be9749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.08 GB: 100%|██████████████████████████████████████████████████████████████| 81/81 [01:41<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\arxiv.zip\n",
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1026.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into DGL objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 205.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=169343, num_edges=1166243,\n",
      "      ndata_schemes={'feat': Scheme(shape=(128,), dtype=torch.float32), 'year': Scheme(shape=(1,), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "# load data\n",
    "dataset=DglNodePropPredDataset(name='ogbn-arxiv')\n",
    "\n",
    "# assign graph and labels\n",
    "g, labels=dataset[0]\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf621dd-94f1-4352-b003-f79fe2169fed",
   "metadata": {
    "id": "0cf621dd-94f1-4352-b003-f79fe2169fed"
   },
   "source": [
    "<a name='s1-2.1'></a>\n",
    "### Exploratory Data Analysis ###\n",
    "\n",
    "We will perform some basic exploratory data analysis to understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc25cec3-3846-4294-9633-53f164327b19",
   "metadata": {
    "id": "fc25cec3-3846-4294-9633-53f164327b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node dict keys: \n",
      "dict_keys(['feat', 'year'])\n",
      "\n",
      "Node feature shape (num_of_nodes x num_of_features): \n",
      "torch.Size([169343, 128])\n",
      "\n",
      "Number of nodes: \n",
      "169343\n",
      "\n",
      "Number of edges: \n",
      "1166243\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# print node keys\n",
    "print(\"Node dict keys: \\n{}\\n\".format(g.ndata.keys()))\n",
    "\n",
    "# print node feature shape\n",
    "print(\"Node feature shape (num_of_nodes x num_of_features): \\n{}\\n\".format(g.ndata['feat'].shape))\n",
    "\n",
    "# print number of nodes\n",
    "print(\"Number of nodes: \\n{}\\n\".format(g.num_nodes()))\n",
    "\n",
    "# print number of edges\n",
    "print(\"Number of edges: \\n{}\".format(g.num_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcf7f572-1a01-4ed4-a81f-c23fd4033b1a",
   "metadata": {
    "id": "bcf7f572-1a01-4ed4-a81f-c23fd4033b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: \n",
      "torch.Size([169343, 1])\n",
      "\n",
      "Label classes: \n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39])\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# print labels\n",
    "print(\"Labels shape: \\n{}\\n\".format(labels.shape))\n",
    "print(\"Label classes: \\n{}\".format(labels.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb597a-7769-4c81-94f6-b33bb8d5e600",
   "metadata": {
    "id": "8bdb597a-7769-4c81-94f6-b33bb8d5e600"
   },
   "source": [
    "<a name='s1-e2'></a>\n",
    "### Exercise #2 - Finding Node With Highest Number of Connections ###\n",
    "Let's find the node with the highest node degree.\n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>` only to find the node with the highest number of connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c990b60-6b6b-4bec-870c-986685abeaa7",
   "metadata": {
    "id": "6c990b60-6b6b-4bec-870c-986685abeaa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node_1353 has the most connections. \n"
     ]
    }
   ],
   "source": [
    "# placeholder for max node id and max connections count\n",
    "max_node_id=0\n",
    "max_count=0\n",
    "\n",
    "# iterate through all nodes\n",
    "for each_node in g.nodes():\n",
    "    # check if number of connections is larger than current max\n",
    "    count=len(g.in_edges(each_node)[0])\n",
    "    if count>max_count:\n",
    "\n",
    "        # set max_count and max_node_id\n",
    "        max_count=count\n",
    "        max_node_id=each_node\n",
    "\n",
    "# print node with most connections\n",
    "print(\"Node_{} has the most connections. \".format(max_node_id))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f70ada07-8a6f-4626-9359-ab5c43833cbe",
   "metadata": {
    "id": "f70ada07-8a6f-4626-9359-ab5c43833cbe",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "max_node_id=0\n",
    "max_count=0\n",
    "\n",
    "for each_node in g.nodes():\n",
    "    count=len(g.in_edges(each_node)[0])\n",
    "    if count>max_count:\n",
    "        max_count=count\n",
    "        max_node_id=each_node\n",
    "\n",
    "print(\"Node_{} has the most connections. \".format(max_node_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f84bd8-b4f9-4b3c-ad4c-8a3c214f4e71",
   "metadata": {
    "id": "91f84bd8-b4f9-4b3c-ad4c-8a3c214f4e71"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Node degrees can also be accessed via `.in_degrees()` or `.out_degrees()`\n",
    "print(\"Node degrees via DGL API: \\n{}\\n\".format(g.in_degrees()))\n",
    "\n",
    "# print node with most connections\n",
    "print(\"Node_{} has the most connections. \".format(torch.argmax(g.in_degrees())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c844e-40ba-439b-a15b-96ac7b51324a",
   "metadata": {
    "id": "cd9c844e-40ba-439b-a15b-96ac7b51324a"
   },
   "source": [
    "<a name='s1-2.2'></a>\n",
    "### Data Preparation and Subgraph ###\n",
    "Before we can use the dataset for machine learning, we will perform data splitting for model validation. The original dataset contains 169,343 nodes and 1,116,243 edges. We will split the data based on the publication dates of the papers. By training the model on older papers, we can then use it to predict the labels for newly-published papers. This could provide tremendous value for the arXiv moderators. Given the range of history our data contains, we will train on papers published until 2017, validate on those published in 2018, and test on those published since 2019. The OGB dataloader conveniently provides the `train`, `valid`, and `test` masks for us directly.\n",
    "\n",
    "Furthermore, we'll first work with a subset of the data for the purpose of demonstration. Specifically, since we will be using the dense adjacency matrix to build our first GNNs, our memory capacity allows only a subset of the nodes. DGL provides a convenient [`dgl.DGLGraph.subgraph(nodes)`](https://docs.dgl.ai/en/0.2.x/generated/dgl.DGLGraph.subgraph.html) function to help us achieve this. We can obtain the node/edge mapping from the subgraph to the original graph by looking into the node feature `dgl.NID` or edge feature `dgl.EID` in the new graph. `subgraph` also copies the original features to the subgraph. Once we understand how GNNs work, we will be able to train with the sparse adjacency matrix for the entire dataset.\n",
    "\n",
    "_Note: While the `ogbn-arxiv` dataset represents a directed graph, we will ignore the edge direction and treat it as undirected._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e032a34d-2ac2-4d5e-aa4d-80c49831fa07",
   "metadata": {
    "id": "e032a34d-2ac2-4d5e-aa4d-80c49831fa07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=18991, num_edges=64740,\n",
      "      ndata_schemes={'feat': Scheme(shape=(128,), dtype=torch.float32), 'year': Scheme(shape=(1,), dtype=torch.int64), '_ID': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Parent node IDs: tensor([     0,      1,      7,  ..., 169338, 169341, 169342])\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "import numpy as np\n",
    "\n",
    "# assign number of labels\n",
    "num_classes=6\n",
    "\n",
    "# get subset of node indices\n",
    "sub_nodes=np.where(np.isin(labels, range(num_classes)))[0]\n",
    "\n",
    "# get subset of labels\n",
    "sub_labels=labels[np.isin(labels, range(num_classes))]\n",
    "\n",
    "# get subgraph and make bidirectional/undirected\n",
    "sub_g=g.subgraph(sub_nodes)\n",
    "sub_g=dgl.to_bidirected(sub_g, copy_ndata=True)\n",
    "print(sub_g)\n",
    "\n",
    "# print parent node IDs\n",
    "parent_nodes=sub_g.ndata[dgl.NID]\n",
    "print(\"Parent node IDs: {}\".format(parent_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ea2c9d6-a829-479f-a1d1-485d46063b10",
   "metadata": {
    "id": "5ea2c9d6-a829-479f-a1d1-485d46063b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11234 nodes for training: \n",
      "3014 nodes for validation: \n",
      "4743 nodes for testing. \n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# get data split\n",
    "split_idx=dataset.get_idx_split()\n",
    "\n",
    "# get train, valid, and test splits\n",
    "train_idx, valid_idx, test_idx=split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "# sample subset for the train, valid, and test set using parent_nodes\n",
    "train_mask=[True if idx in train_idx else False for idx in parent_nodes]\n",
    "valid_mask=[True if idx in valid_idx else False for idx in parent_nodes]\n",
    "test_mask=[True if idx in test_idx else False for idx in parent_nodes]\n",
    "\n",
    "print(\"{} nodes for training: \\n{} nodes for validation: \\n{} nodes for testing. \".format(sum(train_mask), sum(valid_mask), sum(test_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7878d7-5b6a-4624-ac7c-4404870ae616",
   "metadata": {
    "id": "9c7878d7-5b6a-4624-ac7c-4404870ae616"
   },
   "source": [
    "<a name='s1-3'></a>\n",
    "## Building Graph Neural Networks for Node Classification ##\n",
    "Graph data is a challenge as standard deep learning methods focus primarily on structured data, such as fixed-size pixel grids (images) and sequences (text). Graph neural networks, or **GNN**s, refers to a variety of different approaches for applying deep learning on graphs that:\n",
    "* Take full advantage of the graph structure\n",
    "* Considers scalability and efficiency based on the size of the graph and its features\n",
    "* Provide an easy way to do node-level, edge-level, and graph-level prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d1849-9aa3-4ef6-80f6-90956125a4f8",
   "metadata": {
    "id": "700d1849-9aa3-4ef6-80f6-90956125a4f8"
   },
   "source": [
    "<a name='s1-3.1'></a>\n",
    "### Message Passing and Graph Convolution ###\n",
    "**Message passing** is when each node in a graph sends information about itself to its neighbors and receives messages back from them to update its status and understand its environment. It enables graph neural networks to explore a graph's connectivity by correlating messages (features), sent by neighboring nodes, to update a node's features. The message passing feature is the core of DGL library and most graph computations in DGL rely on it. Message passing can be seen as two major steps:\n",
    "1. The first step consists of the \"message\" stage, where the messages are sent over or scattered by the nodes to its neighbors. It is performed for all existing or relevant edges in the graph.  \n",
    "2. The second step is the \"reduce\" stage, where the messages are **aggregated** by the receiving node and used to update its features. It is performed for all existing or relevant nodes in the graph.\n",
    "\n",
    "**Graph [convolution](https://en.wikipedia.org/wiki/Convolution)** combines information from neighborhoods and encodes updated node features to *latent representations*. It can be achieved based on a simple message passing mechanism that involves a linear combination of neighborhood features where weights used for the aggregation depend only on the structure of the graph.\n",
    "* An [embedding](https://en.wikipedia.org/wiki/Embedding) is a relatively low-dimensional space into which we can translate high-dimensional vectors such that similar items are close to each other.\n",
    "\n",
    "We will demonstrate these basic mechanisms through various node classification graph neural networks. Node classification, in simple words, refers to the task of predicting the labels of specific nodes by assessing the features and information from neighboring nodes.\n",
    "\n",
    "<p><img src='images/gnn_node_classification.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f156a8-3505-48da-9d34-e31e4a0ab486",
   "metadata": {
    "id": "05f156a8-3505-48da-9d34-e31e4a0ab486"
   },
   "source": [
    "<a name='s1-4'></a>\n",
    "## Building GNNs With PyTorch ##\n",
    "[PyTorch](https://pytorch.org/) is an open-source machine learning framework based on the Python programming language and the [Torch](http://torch.ch/) library. It is used for creating deep neural networks and very popular for deep learning research. We will first use PyTorch to define the layers of our graph neural networks to demonstrate the basic mechanics. Later in the lab, we will also explore other methods of building graph neural networks.\n",
    "\n",
    "Below we will define a simple two-layer graph neural network to help us explore various approaches. The first layer will apply matrix multiplication across the feature vector and the adjacency matrix, as well as a weight matrix. We will also pass the resulting matrix to a non-linear activation function for deep learning. This layer will allow us to implement various aggregation schemes such as the `sum-pooling`, `mean-pooling`, etc.\n",
    "\n",
    "In addition to our GNN model, we will define an evaluation function and a train function that will allow us to do standard deep learning model training. Since we are training classification models, we will calculate [negative log likelihood loss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) (`nll_loss`) as loss and `accuracy` as the evaluation metric.\n",
    "\n",
    "<p><img src='images/aggregation.png' width=960></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dbbc6aa-cb40-483e-b51f-8277434683aa",
   "metadata": {
    "id": "2dbbc6aa-cb40-483e-b51f-8277434683aa"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5550ade-747a-414f-be75-e91d8b61cb5d",
   "metadata": {
    "id": "b5550ade-747a-414f-be75-e91d8b61cb5d"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define SimpleGraphNet\n",
    "class SimpleGraphNet(nn.Module):\n",
    "    \"\"\"Simple graph neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feats (int): input feature size\n",
    "    h_feats (int): hidden feature size\n",
    "    num_classes (int): number of classes\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        # for inheritance we use super() to refer to the base class\n",
    "        super(SimpleGraphNet, self).__init__()\n",
    "\n",
    "        # two linear layers where each one will have its own weights, W\n",
    "        # first layer computes the hidden layer\n",
    "        self.layer1 = nn.Linear(in_feats, h_feats)\n",
    "        # use num_classes units for the second layer to compute the classification of each node\n",
    "        self.layer2 = nn.Linear(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, h, adj):\n",
    "        \"\"\"Forward computation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g (DGLGraph): the input graph\n",
    "        h (Tensor): the input node features\n",
    "        adj (Tensor): the graph adjacency matrix\n",
    "        \"\"\"\n",
    "        # apply first linear layer's transform weights\n",
    "        x=self.layer1(h)\n",
    "\n",
    "        # perform matrix multiplication with the adjacency matrix and node features to\n",
    "        # aggregate/recombine across neighborhoods\n",
    "        x=torch.mm(adj, x)\n",
    "\n",
    "        # apply a relu activation function\n",
    "        x=F.relu(x)\n",
    "\n",
    "        # apply second linear layer's transform weights\n",
    "        x=self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "450a66a2-bbb7-4e7a-9917-1cabd78640e3",
   "metadata": {
    "id": "450a66a2-bbb7-4e7a-9917-1cabd78640e3"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "import time\n",
    "\n",
    "# define evaluate\n",
    "def evaluate(model, g, adj, labels, mask):\n",
    "    \"\"\"Model evaluation for particular set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model (nn.Module): the model\n",
    "    features (Tensor): the feature tensor\n",
    "    adj (Tensor): the graph adjacency matrix\n",
    "    labels (Tensor): the ground truth labels\n",
    "    mask (Tensor): the mask for a specific subset\n",
    "    \"\"\"\n",
    "    # assign features\n",
    "    features=g.ndata['feat']\n",
    "\n",
    "    # set to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # put features through model to obtain logits\n",
    "        logits=model(g, features, adj)\n",
    "\n",
    "        # get logits and labels for particular set\n",
    "        logits=logits[mask]\n",
    "        labels=labels[mask]\n",
    "\n",
    "        # get most likely class and count the number of corrects\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "\n",
    "        # return accuracy\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9b81874-8b3f-4b47-86cb-7ed784187721",
   "metadata": {
    "id": "c9b81874-8b3f-4b47-86cb-7ed784187721"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define train\n",
    "def train(model, g, adj, labels):\n",
    "    \"\"\"Model training\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model (nn.Module): the model\n",
    "    features (Tensor): the feature tensor\n",
    "    adj (Tensor): the graph adjacency matrix\n",
    "    labels (Tensor): the ground truth labels\n",
    "    \"\"\"\n",
    "    # assign features\n",
    "    features=g.ndata['feat']\n",
    "\n",
    "    # use a standard optimization pipeline using the adam optimizer\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    # standard training pipeline with early stopping\n",
    "    best_acc=0.0\n",
    "    for epoch in range(200):\n",
    "        start=time.time()\n",
    "\n",
    "        # set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # forward step\n",
    "        # calculate logits and loss\n",
    "        logits=model(g, features, adj)\n",
    "        # calculate loss using log_softmax and negative log likelihood\n",
    "        logp=F.log_softmax(logits, 1)\n",
    "        loss=F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "        # backward step\n",
    "        # zero out gradients before accumulating the gradients on backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # apply the optimizer to the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate on validation and test sets\n",
    "        val_acc=evaluate(model, g, adj, labels, valid_mask)\n",
    "        test_acc=evaluate(model, g, adj, labels, test_mask)\n",
    "\n",
    "        # compare validation accuracy with best accuracy at 10 epoch intervals, which will update if exceeded\n",
    "        if (epoch%10==0) & (val_acc>best_acc):\n",
    "            best_acc=val_acc\n",
    "            print(\"Epoch {:03d} | Loss {:.4f} | Validation Acc {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "                epoch, loss.item(), val_acc, test_acc, time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47446a-1cb1-4859-862f-da063cc8324b",
   "metadata": {
    "id": "dc47446a-1cb1-4859-862f-da063cc8324b"
   },
   "source": [
    "<a name='s1-4.1'></a>\n",
    "### Sum-Pooling ###\n",
    "As a starting point, we'll train a simple graph neural network for node classification using the `sum-pooling` aggregation. Recall that `sum-pooling` can cause issues related to scaling of the feature and may not give the best results.\n",
    "\n",
    "<p><img src='images/sum-pooling.PNG' width=240></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f826f40-41a3-4cfa-a80a-852d649bae46",
   "metadata": {
    "id": "0f826f40-41a3-4cfa-a80a-852d649bae46"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create adjacency matrix\n",
    "adj=sub_g.adj().to_dense()\n",
    "\n",
    "# modify the adjacency matrix by adding the identity matrix to ensure nodes consider their own features\n",
    "adj=adj+torch.eye(sub_g.adj().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "addd668e-8f57-46b0-beb6-74615886192e",
   "metadata": {
    "id": "addd668e-8f57-46b0-beb6-74615886192e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGraphNet(\n",
      "  (layer1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (layer2): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n",
      "Epoch 000 | Loss 1.8406 | Validation Acc 0.3822 | Test Acc 0.3761 | Time(s) 1.5360\n",
      "Epoch 010 | Loss 1.0203 | Validation Acc 0.6510 | Test Acc 0.6515 | Time(s) 1.2810\n",
      "Epoch 020 | Loss 0.7643 | Validation Acc 0.7253 | Test Acc 0.7268 | Time(s) 1.3886\n",
      "Epoch 030 | Loss 0.6119 | Validation Acc 0.7817 | Test Acc 0.7748 | Time(s) 1.5598\n",
      "Epoch 040 | Loss 0.5388 | Validation Acc 0.8099 | Test Acc 0.7856 | Time(s) 1.7015\n",
      "Epoch 050 | Loss 0.5023 | Validation Acc 0.8202 | Test Acc 0.7980 | Time(s) 1.7863\n",
      "Epoch 060 | Loss 0.4798 | Validation Acc 0.8278 | Test Acc 0.8075 | Time(s) 1.6571\n",
      "Epoch 070 | Loss 0.4634 | Validation Acc 0.8288 | Test Acc 0.8132 | Time(s) 1.5211\n",
      "Epoch 080 | Loss 0.4509 | Validation Acc 0.8298 | Test Acc 0.8168 | Time(s) 1.3508\n",
      "Epoch 090 | Loss 0.4411 | Validation Acc 0.8328 | Test Acc 0.8178 | Time(s) 1.3933\n",
      "Epoch 110 | Loss 0.4253 | Validation Acc 0.8334 | Test Acc 0.8210 | Time(s) 1.3769\n",
      "Epoch 120 | Loss 0.4187 | Validation Acc 0.8344 | Test Acc 0.8204 | Time(s) 1.3460\n",
      "Epoch 150 | Loss 0.4044 | Validation Acc 0.8354 | Test Acc 0.8229 | Time(s) 1.5044\n",
      "Epoch 190 | Loss 0.3879 | Validation Acc 0.8358 | Test Acc 0.8199 | Time(s) 1.4265\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# instantiate simple model\n",
    "model=SimpleGraphNet(sub_g.ndata['feat'].shape[1], 32, num_classes)\n",
    "\n",
    "# print model architecture\n",
    "print(model)\n",
    "\n",
    "# start training\n",
    "train(model, sub_g, adj, sub_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42ebd9-1da4-424b-b4c7-6c8fd82c9ec8",
   "metadata": {
    "id": "3a42ebd9-1da4-424b-b4c7-6c8fd82c9ec8"
   },
   "source": [
    "<a name='s1-4.2'></a>\n",
    "### Baseline MLP Model ###\n",
    "We can test our model by using an [identify matrix](https://en.wikipedia.org/wiki/Identity_matrix) instead of the adjacency matrix. This is equivalent to creating a standard MLP classificaiton model that shares weights across the vertices. We can use this as a baseline to see how much improvements graph convolution offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "022376c3-c462-4a45-9981-8b6780ba8bca",
   "metadata": {
    "id": "022376c3-c462-4a45-9981-8b6780ba8bca"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# modify the adjacency matrix by adding the identity matrix to ensure nodes consider their own features\n",
    "adj=torch.eye(sub_g.adj().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e94173f-27a5-4874-9dab-17b3cae59507",
   "metadata": {
    "id": "0e94173f-27a5-4874-9dab-17b3cae59507",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGraphNet(\n",
      "  (layer1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (layer2): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n",
      "Epoch 000 | Loss 1.7872 | Validation Acc 0.1835 | Test Acc 0.1678 | Time(s) 1.6565\n",
      "Epoch 010 | Loss 1.1153 | Validation Acc 0.5723 | Test Acc 0.5779 | Time(s) 1.3820\n",
      "Epoch 020 | Loss 0.8261 | Validation Acc 0.6848 | Test Acc 0.6804 | Time(s) 1.4634\n",
      "Epoch 030 | Loss 0.6874 | Validation Acc 0.7216 | Test Acc 0.7004 | Time(s) 1.3581\n",
      "Epoch 040 | Loss 0.6254 | Validation Acc 0.7518 | Test Acc 0.7379 | Time(s) 1.4520\n",
      "Epoch 050 | Loss 0.5861 | Validation Acc 0.7658 | Test Acc 0.7443 | Time(s) 1.9002\n",
      "Epoch 060 | Loss 0.5651 | Validation Acc 0.7684 | Test Acc 0.7472 | Time(s) 1.3732\n",
      "Epoch 090 | Loss 0.5334 | Validation Acc 0.7701 | Test Acc 0.7514 | Time(s) 1.3700\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# instantiate simple model\n",
    "model=SimpleGraphNet(sub_g.ndata['feat'].shape[1], 32, num_classes)\n",
    "\n",
    "# print model architecture\n",
    "print(model)\n",
    "\n",
    "# start training\n",
    "train(model, sub_g, adj, sub_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd827d3-1be2-43c5-b6c7-e0b26e1796a1",
   "metadata": {
    "id": "3fd827d3-1be2-43c5-b6c7-e0b26e1796a1"
   },
   "source": [
    "<a name='s1-e3'></a>\n",
    "### Exercise #3 - Mean-Pooling ###\n",
    "Graph neural networks that use a `mean-pooling` aggregation normalizes the vector to prevent features from exploding since the scale of the output features can increase.\n",
    "\n",
    "<p><img src='images/mean-pooling.PNG' width=240></p>\n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>` only to calculate the number of connections per node.\n",
    "* Execute the cell below to train a GNN for node classification using the `mean-pooling` aggregation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "161d031c-e758-44c0-b4f6-4654e446c85e",
   "metadata": {
    "id": "161d031c-e758-44c0-b4f6-4654e446c85e"
   },
   "outputs": [],
   "source": [
    "# create adjacency matrix\n",
    "adj=sub_g.adj().to_dense()\n",
    "\n",
    "# modify the adjacency matrix by adding the identity matrix to ensure nodes consider their own features\n",
    "adj=adj+torch.eye(sub_g.adj().shape[0])\n",
    "\n",
    "# get node degrees\n",
    "deg=adj.sum(dim=0)\n",
    "\n",
    "# divide the adjacency matrix by the degree matrix, which is equivalent to multiplying it with the\n",
    "# inverse of the degree matrix. This gives a normalize propagation rule, which should hopefully deal with\n",
    "# any exploding signal that we might have\n",
    "adj=adj/deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0d88278-3177-4a76-9456-d358598323cf",
   "metadata": {
    "id": "f0d88278-3177-4a76-9456-d358598323cf",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGraphNet(\n",
      "  (layer1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (layer2): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n",
      "Epoch 000 | Loss 1.7563 | Validation Acc 0.4519 | Test Acc 0.4853 | Time(s) 1.4909\n",
      "Epoch 010 | Loss 1.1428 | Validation Acc 0.6065 | Test Acc 0.6165 | Time(s) 1.3557\n",
      "Epoch 020 | Loss 0.7939 | Validation Acc 0.7213 | Test Acc 0.7301 | Time(s) 1.3438\n",
      "Epoch 030 | Loss 0.6100 | Validation Acc 0.7664 | Test Acc 0.7567 | Time(s) 1.3381\n",
      "Epoch 040 | Loss 0.5093 | Validation Acc 0.8106 | Test Acc 0.7965 | Time(s) 1.3561\n",
      "Epoch 050 | Loss 0.4621 | Validation Acc 0.8305 | Test Acc 0.8121 | Time(s) 1.4087\n",
      "Epoch 060 | Loss 0.4393 | Validation Acc 0.8368 | Test Acc 0.8153 | Time(s) 1.3390\n",
      "Epoch 070 | Loss 0.4247 | Validation Acc 0.8411 | Test Acc 0.8178 | Time(s) 1.3713\n",
      "Epoch 080 | Loss 0.4140 | Validation Acc 0.8414 | Test Acc 0.8183 | Time(s) 1.4419\n",
      "Epoch 090 | Loss 0.4062 | Validation Acc 0.8427 | Test Acc 0.8199 | Time(s) 1.3349\n",
      "Epoch 100 | Loss 0.4000 | Validation Acc 0.8431 | Test Acc 0.8189 | Time(s) 1.3996\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# instantiate simple model\n",
    "model=SimpleGraphNet(sub_g.ndata['feat'].shape[1], 32, num_classes)\n",
    "\n",
    "# print model architecture\n",
    "print(model)\n",
    "\n",
    "# start training\n",
    "train(model, sub_g, adj, sub_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9bc4fd2-4f5c-4998-8812-5b267e3e0aea",
   "metadata": {
    "id": "b9bc4fd2-4f5c-4998-8812-5b267e3e0aea",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "adj=sub_g.adj().to_dense()\n",
    "adj=adj+torch.eye(sub_g.adj().shape[0])\n",
    "deg=adj.sum(dim=0)\n",
    "adj=adj/deg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece35ff-373d-4cc1-9fc7-a2324f12e793",
   "metadata": {
    "id": "fece35ff-373d-4cc1-9fc7-a2324f12e793"
   },
   "source": [
    "<a name='s1-4.3'></a>\n",
    "### GCN, Graph Convolutional Network ###\n",
    "The commonly cited node classification graph convolutional network (GCN), as proposed by Kipf and Welling ([arXiv](https://arxiv.org/abs/1609.02907)), uses symmetric normalization in the update rule. It involves multiplying the learnable function by the inverse square root of the degree matrix from both sides, which is analogous to dividing by the square root of the product of neighborhood sizes of a node and neighborhood sizes of the neighbor.\n",
    "\n",
    "<p><img src='images/gcn.PNG' width=240></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97fded42-faf7-4c86-8c7d-f8db2716a012",
   "metadata": {
    "id": "97fded42-faf7-4c86-8c7d-f8db2716a012"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create adjacency matrix\n",
    "adj=sub_g.adj().to_dense()\n",
    "\n",
    "# modify the adjacency matrix by adding the identity matrix to ensure nodes consider their own features\n",
    "adj=adj+torch.eye(sub_g.adj().shape[0])\n",
    "\n",
    "# get node degrees\n",
    "deg=adj.sum(dim=0)\n",
    "\n",
    "# normalization computes 1 over the square root of the degree matrix\n",
    "# multiply that on both sides with the adjacency matrix\n",
    "norm_deg=torch.diag(1.0/torch.sqrt(deg))\n",
    "# get the normalized adjacency matrix by multiplying the normalized degree matrix with\n",
    "# the product of the adjacency matrix and the normalized degree matrix\n",
    "norm_adj=torch.mm(norm_deg, torch.matmul(adj, norm_deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1454bbbc-59ae-4c78-b3d3-9de58f8e737e",
   "metadata": {
    "id": "1454bbbc-59ae-4c78-b3d3-9de58f8e737e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGraphNet(\n",
      "  (layer1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (layer2): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n",
      "Epoch 000 | Loss 1.8285 | Validation Acc 0.1974 | Test Acc 0.1868 | Time(s) 1.5890\n",
      "Epoch 010 | Loss 1.1507 | Validation Acc 0.6267 | Test Acc 0.6369 | Time(s) 1.3288\n",
      "Epoch 020 | Loss 0.8069 | Validation Acc 0.6898 | Test Acc 0.7017 | Time(s) 1.3618\n",
      "Epoch 030 | Loss 0.6067 | Validation Acc 0.7804 | Test Acc 0.7723 | Time(s) 1.3517\n",
      "Epoch 040 | Loss 0.5096 | Validation Acc 0.8079 | Test Acc 0.7915 | Time(s) 1.3987\n",
      "Epoch 050 | Loss 0.4575 | Validation Acc 0.8278 | Test Acc 0.8155 | Time(s) 1.3600\n",
      "Epoch 060 | Loss 0.4220 | Validation Acc 0.8394 | Test Acc 0.8235 | Time(s) 1.5847\n",
      "Epoch 070 | Loss 0.4027 | Validation Acc 0.8431 | Test Acc 0.8277 | Time(s) 1.7611\n",
      "Epoch 080 | Loss 0.3908 | Validation Acc 0.8444 | Test Acc 0.8277 | Time(s) 1.6749\n",
      "Epoch 090 | Loss 0.3822 | Validation Acc 0.8467 | Test Acc 0.8269 | Time(s) 1.6008\n",
      "Epoch 100 | Loss 0.3756 | Validation Acc 0.8474 | Test Acc 0.8273 | Time(s) 1.7433\n",
      "Epoch 110 | Loss 0.3702 | Validation Acc 0.8477 | Test Acc 0.8288 | Time(s) 1.6266\n",
      "Epoch 130 | Loss 0.3615 | Validation Acc 0.8507 | Test Acc 0.8280 | Time(s) 1.4255\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# instantiate simple model\n",
    "model=SimpleGraphNet(sub_g.ndata['feat'].shape[1], 32, num_classes)\n",
    "\n",
    "# print model architecture\n",
    "print(model)\n",
    "\n",
    "# start training\n",
    "train(model, sub_g, norm_adj, sub_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee6070-1912-4de2-8f63-b80203772212",
   "metadata": {
    "id": "cbee6070-1912-4de2-8f63-b80203772212"
   },
   "source": [
    "<a name='s1-5'></a>\n",
    "## Building GNNs with DGL's Built-In Modules ##\n",
    "DGL provides implementation of many popular GNN layers. They all can be invoked easily with one line of code. The full list of supported graph convolution modules can be found [here](https://docs.dgl.ai/api/python/nn-pytorch.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e33d77-2569-43a7-93f9-dc3cc5016884",
   "metadata": {
    "id": "04e33d77-2569-43a7-93f9-dc3cc5016884"
   },
   "source": [
    "<a name='s1-5.1'></a>\n",
    "### GraphConv ###\n",
    "Below we will use the [GraphConv](https://docs.dgl.ai/en/0.9.x/generated/dgl.nn.pytorch.conv.GraphConv.html) module from DGL to implement a 3-layer GCN that utilizes mean normalization like we did above. By stacking `N` GCN layers, the feature representations are updated with information of nodes up to `N` hops away. This is often treated as a hyperparameter for model tuning.\n",
    "\n",
    "_Note: The `in_feats`, `out_feats`, `norm` arguments in `GraphConv` are all we need to consider with this simple approach. Furthermore, we use the same `h_feat` hidden size of all 3 layers. For the `norm` argument, we can use `right` to divide the aggregated messages by each node’s in-degrees, which is equivalent to averaging the received messages. Alternatively, we can use `none`, where no normalization is applied, or `both` (default), where the messages are scaled using symmetric normalization._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c550a43-1654-4891-b2e0-773001061256",
   "metadata": {
    "id": "7c550a43-1654-4891-b2e0-773001061256"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "# define GCN model\n",
    "class BuiltinGCN(nn.Module):\n",
    "    \"\"\"Graph convolutional network using DGL supported graph convolution modules\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feats (int): input feature size\n",
    "    h_feats (int): hidden feature size\n",
    "    num_classes (int): number of classes\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feat, h_feat, num_classes):\n",
    "        super(BuiltinGCN, self).__init__()\n",
    "        self.layer1=GraphConv(in_feat, h_feat, norm='right')\n",
    "        self.layer2=GraphConv(h_feat, h_feat, norm='right')\n",
    "        self.layer3=GraphConv(h_feat, num_classes, norm='right')\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        \"\"\"Forward computation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g (DGLGraph): the input graph\n",
    "        features (Tensor): the input node features\n",
    "        \"\"\"\n",
    "        h=self.layer1(g, h)\n",
    "        h=F.relu(h)\n",
    "        h=self.layer2(g, h)\n",
    "        h=F.relu(h)\n",
    "        h=self.layer3(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d7e3f14-3a73-4f9f-a4c2-87642b816b2b",
   "metadata": {
    "id": "6d7e3f14-3a73-4f9f-a4c2-87642b816b2b"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "import time\n",
    "\n",
    "# define evaluate\n",
    "def evaluate(model, g, labels, mask):\n",
    "    \"\"\"Model evaluation for particular set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model (nn.Module): the model\n",
    "    g (DGLGraph): the input graph\n",
    "    labels (Tensor): the ground truth labels\n",
    "    mask (Tensor): the mask for a specific subset\n",
    "    \"\"\"\n",
    "    # assign features\n",
    "    features=g.ndata['feat']\n",
    "\n",
    "    # set to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # put features through model to obtain logits\n",
    "        logits=model(g, features)\n",
    "\n",
    "        # get logits and labels for particular set\n",
    "        logits=logits[mask]\n",
    "        labels=labels[mask]\n",
    "\n",
    "        # get most likely class and count the number of corrects\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "\n",
    "        # return accuracy\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "789c5cf1-852c-4240-acdf-bf4e6afb542b",
   "metadata": {
    "id": "789c5cf1-852c-4240-acdf-bf4e6afb542b"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define train\n",
    "def train(model, g, labels):\n",
    "    \"\"\"Model training\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model (nn.Module): the model\n",
    "    features (Tensor): the feature tensor\n",
    "    labels (Tensor): the ground truth labels\n",
    "    \"\"\"\n",
    "    # assign features\n",
    "    features=g.ndata['feat']\n",
    "\n",
    "    # use a standard optimization pipeline using the adam optimizer\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    # standard training pipeline with early stopping\n",
    "    best_acc=0.0\n",
    "    for epoch in range(200):\n",
    "        start=time.time()\n",
    "\n",
    "        # set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # forward step\n",
    "        # calculate logits and loss\n",
    "        logits=model(g, features)\n",
    "        # calculate loss using log_softmax and negative log likelihood\n",
    "        logp=F.log_softmax(logits, 1)\n",
    "        loss=F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "        # backward step\n",
    "        # zero out gradients before accumulating the gradients on backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # apply the optimizer to the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate on validation and test sets\n",
    "        val_acc=evaluate(model, g, labels, valid_mask)\n",
    "        test_acc=evaluate(model, g, labels, test_mask)\n",
    "\n",
    "        # compare validation accuracy with best accuracy at 10 epoch intervals, which will update if exceeded\n",
    "        if (epoch%10==0) & (val_acc>best_acc):\n",
    "            best_acc=val_acc\n",
    "            print(\"Epoch {:03d} | Loss {:.4f} | Validation Acc {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "                epoch, loss.item(), val_acc, test_acc, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23840621-a0b8-4ec0-ab08-a0f0bc3cd184",
   "metadata": {
    "id": "23840621-a0b8-4ec0-ab08-a0f0bc3cd184",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BuiltinGCN(\n",
      "  (layer1): GraphConv(in=128, out=32, normalization=right, activation=None)\n",
      "  (layer2): GraphConv(in=32, out=32, normalization=right, activation=None)\n",
      "  (layer3): GraphConv(in=32, out=40, normalization=right, activation=None)\n",
      ")\n",
      "Epoch 000 | Loss 3.6953 | Validation Acc 0.1360 | Test Acc 0.1370 | Time(s) 0.4185\n",
      "Epoch 010 | Loss 1.2677 | Validation Acc 0.5249 | Test Acc 0.5349 | Time(s) 0.2702\n",
      "Epoch 020 | Loss 0.8177 | Validation Acc 0.7508 | Test Acc 0.7480 | Time(s) 0.3146\n",
      "Epoch 030 | Loss 0.5845 | Validation Acc 0.7913 | Test Acc 0.7854 | Time(s) 0.2936\n",
      "Epoch 040 | Loss 0.4499 | Validation Acc 0.8285 | Test Acc 0.8206 | Time(s) 0.2647\n",
      "Epoch 050 | Loss 0.4007 | Validation Acc 0.8461 | Test Acc 0.8396 | Time(s) 0.2838\n",
      "Epoch 060 | Loss 0.3692 | Validation Acc 0.8583 | Test Acc 0.8465 | Time(s) 0.2926\n",
      "Epoch 070 | Loss 0.3515 | Validation Acc 0.8607 | Test Acc 0.8507 | Time(s) 0.3401\n",
      "Epoch 080 | Loss 0.3403 | Validation Acc 0.8610 | Test Acc 0.8511 | Time(s) 0.5331\n",
      "Epoch 090 | Loss 0.3313 | Validation Acc 0.8643 | Test Acc 0.8490 | Time(s) 0.2608\n",
      "Epoch 110 | Loss 0.3180 | Validation Acc 0.8646 | Test Acc 0.8493 | Time(s) 0.2887\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# instantiate GNN model using built-in GraphConv layers\n",
    "model=BuiltinGCN(sub_g.ndata['feat'].shape[1], 32, len(labels.unique()))\n",
    "\n",
    "# add self-loop to ensure nodes consider their own features\n",
    "sub_g=dgl.add_self_loop(sub_g)\n",
    "\n",
    "# print model architecture\n",
    "print(model)\n",
    "\n",
    "# start training\n",
    "train(model, sub_g, sub_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e0ef9-3825-4b61-9fb8-55416dd62627",
   "metadata": {
    "id": "3e4e0ef9-3825-4b61-9fb8-55416dd62627"
   },
   "source": [
    "**Well Done!** Good job on completing the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47e4a7-0661-4d6b-93cc-372f44babba2",
   "metadata": {
    "id": "bd47e4a7-0661-4d6b-93cc-372f44babba2"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
